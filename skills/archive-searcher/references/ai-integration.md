# Reference: AI & Autonomous Search Agents

## Introduction
The 2026 search landscape is dominated by **Semantic Discovery**. Literal string matching is now augmented by AI models like the **GPT-5 family** and **o3-deep-research**, which understand the *intent* behind the search queries within an archive.

## ü§ñ The GPT-5 Search Workflow
GPT-5 models possess a 1M+ token context window, allowing for "Deep Index Reading". Instead of piping small snippets, we can now provide entire file trees for analysis.

### Semantic Search vs. Grep
- **Grep**: "Find where `AUTH_SECRET` is used."
- **GPT-5**: "Find where the authentication secret is initialized and verify if it follows the Edge-compatible pattern required by Auth.js v5."

## üî¨ o3-deep-research for Archive Analysis
The `o3-deep-research` models are specialized in long-running autonomous tasks. When tasked with "Searching the Archive," these models don't just return matches; they build a graph of dependencies.

### Autonomous Agent Loop
1. **Discovery**: Agent runs `rg --json` to find initial matches.
2. **Expansion**: Agent identifies related files (imports) and reads them.
3. **Synthesis**: Agent uses `o3-deep-research` to explain the evolution of the searched feature.

## üõ†Ô∏è Implementation: AI-Augmented Search Script
This Python snippet demonstrates how an autonomous agent might interface with the `archive-searcher` skill.

```python
import subprocess
import json

def autonomous_search(query, path="./"):
    # Step 1: High-speed literal search
    result = subprocess.run(
        ["rg", query, "--json", path], 
        capture_output=True, text=True
    )
    
    matches = [json.loads(line) for line in result.stdout.splitlines() if line]
    
    # Step 2: Feed to o3-deep-research for semantic analysis
    # (Pseudo-code for AI API call)
    context = pack_matches_for_ai(matches)
    analysis = ai.call(
        model="o3-deep-research",
        prompt=f"Based on these archive matches, explain the system's {query} logic.",
        context=context
    )
    
    return analysis
```

## üìà Performance with 2026 Models
| Task | Traditional (Grep) | AI-Augmented (GPT-5) |
| :--- | :--- | :--- |
| Literal Match | 0.01s | 0.5s |
| Bug Root Cause | Manual (Hours) | 2m (Autonomous) |
| API Migration Audit | Manual (Days) | 15m (o3-deep-research) |

## üß© Context Packing for Search Agents
To maximize the efficiency of autonomous agents, use **Repomix-style packing**. This reduces token waste by removing noise and focusing on the logical structure of the archive.

### Recommended Packing Strategy
1. Exclude `dist`, `build`, and `node_modules`.
2. Include `.env.example` and `README.md` for environmental context.
3. Include specific `references/` directories from related skills.

## üö´ The "Do Not" List for AI Search
- **DO NOT** feed raw binary data to the LLM. It will hallucinate content.
- **DO NOT** assume the AI knows about local files not included in the provided context.
- **DO NOT** use AI for simple literal searches that `ripgrep` can handle in milliseconds. Save the AI's "brain power" for synthesis and logic.

## üîÆ The Future: Real-time Autonomous Indexing
By late 2026, autonomous agents will maintain their own vector embeddings of the `conductor/archive/` directory, allowing for near-instant semantic lookups without re-reading the files for every query.

*Updated: January 22, 2026 - 15:20*
